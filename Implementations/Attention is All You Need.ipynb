{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "[last updated: 23/03/2025]\n",
    "1. Blog and line-by-line explanation of transformers architecture\n",
    "2. Separate files for machine translation testing\n",
    "3. ADD: unitesting of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import spacy\n",
    "import GPUtil\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# torchtext imports\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import List, Tuple, Iterator\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            d_model, num_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads     # Dimension of each head's key/query/value\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, \n",
    "                q, k, v,\n",
    "                mask = None,\n",
    "    ):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # Reshape to separate attn_heads\n",
    "        # [batch, seq_len (or token_seq), d_model\\ --> [batch, heads, seq_len, d_k]\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        # Mathematically: \n",
    "        # Dimension: [batch_size, heads, d_k, seq_len]\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # apply mask for padding future tokens\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(\n",
    "                mask==0, 1e-9,\n",
    "            )\n",
    "        \n",
    "        # we calculate softmax along dim: -1 (seq_len)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # infomation aggregation: each pos becomes a weighted sum of values from all positions it attends to\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # Combining all the heads\n",
    "        # [batch, heads, seq_len, d_k] -> [batch, seq_len, heads, d_k] --> [batch, seq_len, d_model] \n",
    "        # ealier we had split `d_model` >> heads x d_k\n",
    "        # Now converting back to `d_model`\n",
    "        # [new] `contiguous`` is used to ensure the entire tensor in a single block of memory\n",
    "        output = output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "\n",
    "        output = self.out_linear(output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalwiseFeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model, d_ff):\n",
    "        super().__init__()\n",
    "        # LL1 -> extands dimension to 4*d_model\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        # LL2 -> projects nacl tp og_dim\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(\n",
    "            self.relu(\n",
    "                self.linear1(x)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Since the transformer architecture cantains no recurrence or convolution  it has non inheret way to understand the order of token in sequence\n",
    "\n",
    "    PosEnc injects information about relative positioning through a linear function in the embedding (positional encoding)\n",
    "\n",
    "    PE(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000 ^ (2i / d_model))\n",
    "\n",
    "    Research Advancements:\n",
    "    - Rotary Positional Embedding (RoPE): integrate PosEnc into attention mech\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model, max_seq_len = 5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Division Term tensor create different frequencies with each each dimension of model gets a frequency. \n",
    "        # For lower_dim get slower freq\n",
    "        # higher_dim get faster freq\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Applying sin to even indicies in embedding dim\n",
    "        pe[:, 0: :2] = torch.sin(position * div_term)\n",
    "        # Applying cos to odd indicies in embedding dim\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register Buffer: PE is not trained but saved and restored in state_dict\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # og_paper: adds these rather than concatenating\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Each encoder layer consists of \n",
    "    1. Mutl-ead self-attention layer\n",
    "    2. Position-wise feed forward network\n",
    "\n",
    "    Each layer followed by a residual connection and layer normalization\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionalwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "        # Layer_norm is used over Batch_norn\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x += self.dropout(attn_output)      # Residual Connection\n",
    "        x = self.norm1(x)                   # normalizatio\n",
    "\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x += self.dropout(x)                # Residual Connection\n",
    "        x = self.norm2(x)                   # Normalization\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Each Decoder Layer consists of\n",
    "    1. Masked multi-head self-attention mech (to prevent attending to future position)\n",
    "    2. Mulihead cross attention over encoder output\n",
    "    3. Position wise feed forward network\n",
    "\n",
    "    * Each sub-layer uses residual connection and normalization\n",
    "    * self-attention mask ensures autotressive property\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionalwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "        # Layer_norm is used over Batch_norn\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, enc_output, self_mask = None, cross_mask = None):\n",
    "        self_attn_output, _ = self.self_attn(x, x, x, self_mask)\n",
    "        x += self.dropout(self_attn_output)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Cross attention: allows decoder to attend to all encoder positions\n",
    "        cross_attn_output, _ = self.cross_attn(x, enc_output, enc_output, cross_mask)\n",
    "        x += self.dropout(cross_attn_output)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        ff_ouput = self.feed_forward(x)\n",
    "        x += self.dropout(ff_ouput)\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder consists of N-layers where ouput from previous layer is input to the next layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stacking indentical encoder layers\n",
    "        self.layers  = nn.ModuleList(\n",
    "            EncoderLayer(\n",
    "                d_model, num_heads, d_ff, dropout\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, \n",
    "                x, mask= None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder consists of N identical layers with the output of each layer serves as input to the next. Then a final layer normalization is applied to last decoder layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model, num_heads, d_ff, num_layer, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layer)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, enc_output, self_mask = None, cross_mask = None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, self_mask, cross_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    - Encoder only moderl: BERT > excel at task understanding\n",
    "    - Decoder only model: GPT > excel at general tasks\n",
    "    - Encoder Decoder: T5, BART > seq2seq tasks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 src_vocab_size, tgt_vocab_size, \n",
    "                 d_model=512, num_heads=8, d_ff=2048, num_layers=6, \n",
    "                 dropout=0.1, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Converting token indicies to embedding\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # Adding Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        # Encoder & Decoder stack\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "\n",
    "        # Linear Layer: project decoder output to vocabulary probabilities\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        # Initialize parameters with Xavier\n",
    "        self._init_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def create_masks(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Padding and sequence mask for attention\n",
    "        1. Padding mask: prevents attending to padding tokens (0s in input)\n",
    "        2. Look-ahead mask: prevent decoder from attenting to future positions\n",
    "        \"\"\"\n",
    "        \n",
    "        # Padding mask for src\n",
    "        # Creating boolean maks that's true for real token and False for paddin tokens\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Look-ahead mask for target\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Look-ahead mask ensures autoregressive property and creates an upper triangualr matrix of Flase values thereby preventing each position from attending to future position\n",
    "        seq_len = tgt.size(1)\n",
    "        look_ahead_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len), \n",
    "            diagonal=1\n",
    "        ).bool()\n",
    "        look_ahead_mask = look_ahead_mask.to(tgt.device)\n",
    "\n",
    "        # Combine padding and look-ahead masks\n",
    "        # Position is valid for attention if both non-padding token AND not in future\n",
    "        tgt_mask = tgt_mask & ~look_ahead_mask\n",
    "\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.create_masks(src, tgt)\n",
    "\n",
    "        # in og_paper, embedding and pos_enc for src_seq are sclaed by sqrt(d_model)\n",
    "        src_embedded = self.dropout(\n",
    "            self.positional_encoding(\n",
    "                self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        tgt_embedded = self.dropout(\n",
    "            self.positional_encoding(\n",
    "                self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        enc_output = self.encoder(src_embedded, src_mask)\n",
    "        dec_output = self.decoder(tgt_embedded, enc_output, tgt_mask, src_mask)\n",
    "\n",
    "        # Project output to vocabulary space to get logits\n",
    "        output = self.linear(dec_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usuage: random\n",
    "def create_transformer_model(\n",
    "        src_vocab_size = 10000, tgt_vocab_size = 10000\n",
    "):\n",
    "    \"\"\"\n",
    "    Standard architecture \n",
    "    - d_model = 512\n",
    "    - num_heads = 8\n",
    "    - d_ff = 2048\n",
    "    - num_layers = 6\n",
    "    - dropout = 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    model = Transformer(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        num_layers=6,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_transformer_model()\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "\n",
    "# Test with random tensors to verify dimensions\n",
    "src = torch.randint(1, 10000, (2, 20))  # [batch_size, seq_len]\n",
    "tgt = torch.randint(1, 10000, (2, 15))  # [batch_size, seq_len]\n",
    "\n",
    "# Forward pass\n",
    "output = model(src, tgt)\n",
    "print(f\"Output shape: {output.shape}\")  # Should be [batch_size, tgt_seq_len, tgt_vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # Download the dataset - this will use English to German by default\n",
    "    train_iter, valid_iter, test_iter = Multi30k(split=('train', 'valid', 'test'))\n",
    "    return train_iter, valid_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizers():\n",
    "    try:\n",
    "        spacy_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "        spacy_de = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "        def tokenizer_en(text):\n",
    "            return [token.lower() for token in spacy_en(text)]\n",
    "        \n",
    "        def tokenizer_de(text):\n",
    "            return [token.lower() for token in spacy_de(text)]\n",
    "        \n",
    "        return tokenizer_en, tokenizer_de\n",
    "    except ImportError:\n",
    "        \n",
    "        print(\"SpaCy model not found. Using basic tokenization\")\n",
    "        basic_en = get_tokenizer('basic_english')\n",
    "        basic_de = lambda text : text.lower().split()\n",
    "\n",
    "        return basic_en, basic_de\n",
    "\n",
    "def build_vocabularies(train_iter, tokenize_src, tokenize_tgt):\n",
    "\n",
    "    # Helper: yield token\n",
    "    def yield_tokens(data_iter, tokenizer, index):\n",
    "        for data_sample in data_iter:\n",
    "            yield tokenizer(data_sample[index])\n",
    "    \n",
    "    special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "    # Building src vocab\n",
    "    train_iter_for_src, _ = Multi30k(split='train')\n",
    "    src_vocab = build_vocab_from_iterator(\n",
    "        yield_tokens(train_iter_for_src, tokenize_src, 0), \n",
    "        min_freq=2, specials=special_symbols\n",
    "    )\n",
    "\n",
    "    # Build target vocabulary\n",
    "    train_iter_for_tgt, _ = Multi30k(split=('train'))\n",
    "    tgt_vocab = build_vocab_from_iterator(\n",
    "        yield_tokens(train_iter_for_tgt, tokenize_tgt, 1),\n",
    "        min_freq=2,\n",
    "        specials=special_symbols\n",
    "    )\n",
    "\n",
    "    # Set Default index to <unk>\n",
    "    src_vocab.set_default_index(src_vocab['<unk>'])\n",
    "    tgt_vocab.set_default_index(tgt_vocab['<unk>'])\n",
    "\n",
    "    return src_vocab, tgt_vocab\n",
    "\n",
    "def process_data(data_iterator, tokenize_src, tokenize_tgt, src_vocab, tgt_vocab):\n",
    "    BOS_IDX = src_vocab['<bos>']\n",
    "    EOS_IDX = src_vocab['<eos>']\n",
    "    PAD_IDX = src_vocab['<pad>']\n",
    "\n",
    "    data_pairs = []\n",
    "    for sample in data_iterator:\n",
    "        src_text, tgt_text = sample\n",
    "\n",
    "        # Tokenize and convert to indices\n",
    "        src_tokens = [BOS_IDX] + [src_vocab[token] for token in tokenize_src(src_text)] + [EOS_IDX]\n",
    "        tgt_tokens = [BOS_IDX] + [tgt_vocab[token] for token in tokenize_tgt(tgt_text)] + [EOS_IDX]\n",
    "        \n",
    "        data_pairs.append((torch.tensor(src_tokens), torch.tensor(tgt_tokens)))\n",
    "    \n",
    "    return data_pairs\n",
    "\n",
    "def create_batch(data_batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_item, tgt_item in data_batch:\n",
    "        src_batch.append(src_item)\n",
    "        tgt_batch.append(tgt_item)\n",
    "    \n",
    "    # Pad sequences in batch to the same length\n",
    "    src_batch = pad_sequence(src_batch, padding_value=src_vocab['<pad>'], batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=tgt_vocab['<pad>'], batch_first=True)\n",
    "    \n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "def evaluate(model, val_loader, criterion, pad_idx):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (src, tgt) in enumerate(val_loader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # target sequence without the last token\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            \n",
    "            # Targets for loss calculation (without the BOS token)\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(src, tgt_input)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            outputs = outputs.contiguous().view(-1, outputs.shape[-1])\n",
    "            tgt_output = tgt_output.contiguous().view(-1)\n",
    "            \n",
    "            # Calculate loss (ignoring padding)\n",
    "            loss = criterion(outputs, tgt_output)\n",
    "            losses += loss.item()\n",
    "    \n",
    "    return losses / len(val_loader)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, pad_idx):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # target sequence without the last token\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            # Targets for loss calculation (without the BOS token)\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            # Forward pass\n",
    "            outputs = model(src, tgt_input)\n",
    "            # Reshape for loss calculation\n",
    "            outputs = outputs.contiguous().view(-1, outputs.shape[-1])\n",
    "            tgt_output = tgt_output.contiguous().view(-1)\n",
    "            # Calculate loss (ignoring padding)\n",
    "            loss = criterion(outputs, tgt_output)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Print batch statistics\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        epoch_loss = epoch_loss / len(train_loader)\n",
    "        val_loss = evaluate(model, val_loader, criterion, pad_idx)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "        print(f'Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_transformer_model.pt')\n",
    "            print(f'Best model saved with validation loss: {best_val_loss:.4f}')\n",
    "\n",
    "def translate_sentence(model, sentence, tokenize_src, src_vocab, tgt_vocab, max_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = tokenize_src(sentence)\n",
    "    tokens = ['<bos>'] + tokens + ['<eos>']                             # Add BOS and EOS tokens\n",
    "    src_indices = [src_vocab[token] for token in tokens]                # Convert to indices\n",
    "    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)  # Convert to tensor and add batch dimension\n",
    "    \n",
    "    # Initialize target with BOS token\n",
    "    tgt_indices = [tgt_vocab['<bos>']]\n",
    "    \n",
    "    # Get special token indices\n",
    "    eos_idx = tgt_vocab['<eos>']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode the source sentence\n",
    "        encoder_outputs = model.encoder(\n",
    "            model.dropout(\n",
    "                model.positional_encoding(\n",
    "                    model.src_embedding(src_tensor) * math.sqrt(model.d_model)\n",
    "                )\n",
    "            ),\n",
    "            src_mask=None\n",
    "        )\n",
    "        \n",
    "        # Initialize with BOS token\n",
    "        output = torch.LongTensor([tgt_vocab['<bos>']]).to(device)\n",
    "        \n",
    "        # Generate tokens auto-regressively\n",
    "        for i in range(max_len):\n",
    "            # Prepare target tensor (so far)\n",
    "            tgt_tensor = output.unsqueeze(0)\n",
    "            \n",
    "            # Create masks\n",
    "            src_mask = (src_tensor != src_vocab['<pad>']).unsqueeze(1).unsqueeze(2).to(device)\n",
    "            tgt_mask = model.generate_subsequent_mask(tgt_tensor.size(1)).to(device)\n",
    "            \n",
    "            # Pass through decoder\n",
    "            tgt_embedded = model.dropout(\n",
    "                model.positional_encoding(\n",
    "                    model.tgt_embedding(tgt_tensor) * math.sqrt(model.d_model)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            decoder_output = model.decoder(\n",
    "                tgt_embedded, \n",
    "                encoder_outputs, \n",
    "                tgt_mask=tgt_mask, \n",
    "                src_mask=src_mask\n",
    "            )\n",
    "            \n",
    "            # Get prediction\n",
    "            prediction = model.linear(decoder_output)\n",
    "            pred_token = prediction[0, -1].argmax().item()\n",
    "            \n",
    "            # Add predicted token to output\n",
    "            output = torch.cat([output, torch.LongTensor([pred_token]).to(device)])\n",
    "            \n",
    "            # Break if EOS token is predicted\n",
    "            if pred_token == eos_idx:\n",
    "                break\n",
    "    \n",
    "    # Convert indices back to tokens\n",
    "    tgt_tokens = [tgt_vocab.get_itos()[i] for i in output]\n",
    "    \n",
    "    # Remove special tokens and return translation\n",
    "    return ' '.join(tgt_tokens[1:-1])  # Remove BOS and EOS\n",
    "\n",
    "# Helper: generate look-ahead mask for decoder\n",
    "def generate_subsequent_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
    "    return mask.to(device)\n",
    "\n",
    "Transformer.generate_subsequent_mask = generate_subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_iter, valid_iter, test_iter = load_dataset()\n",
    "    tokenize_src, tokenize_tgt = get_tokenizers()\n",
    "\n",
    "    global src_vocab, tgt_vocab  \n",
    "    src_vocab, tgt_vocab = build_vocabularies(train_iter, tokenize_src, tokenize_tgt)\n",
    "    \n",
    "    train_data = process_data(train_iter, tokenize_src, tokenize_tgt, src_vocab, tgt_vocab)\n",
    "    valid_data = process_data(valid_iter, tokenize_src, tokenize_tgt, src_vocab, tgt_vocab)\n",
    "    test_data = process_data(test_iter, tokenize_src, tokenize_tgt, src_vocab, tgt_vocab)\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=create_batch)\n",
    "    valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=create_batch)\n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=create_batch)\n",
    "    \n",
    "    src_vocab_size = len(src_vocab)\n",
    "    tgt_vocab_size = len(tgt_vocab)\n",
    "    \n",
    "    # Standard hyperparameters\n",
    "    D_MODEL = 512\n",
    "    NUM_HEADS = 8\n",
    "    NUM_LAYERS = 6\n",
    "    D_FF = 2048\n",
    "    DROPOUT = 0.1\n",
    "\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 0.0001\n",
    "    # lr = D_MODEL**-0.5 * min(step_num**-0.5, step_num * WARMUP_STEPS**-1.5)\n",
    "    \n",
    "    model = Transformer(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        d_ff=D_FF,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(device)\n",
    "    \n",
    "    # Stepup: raining parameters\n",
    "    PAD_IDX = src_vocab['<pad>']\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) \n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
    "    \n",
    "    train_model(model, train_loader, valid_loader, optimizer, criterion, NUM_EPOCHS, PAD_IDX)\n",
    "    \n",
    "    # Evaluation: Test dataset\n",
    "    test_loss = evaluate(model, test_loader, criterion, PAD_IDX)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    \n",
    "    # Inference: sample query\n",
    "    sample_sentence = \"The young boy is playing soccer in the park.\"\n",
    "    translation = translate_sentence(model, sample_sentence, tokenize_src, src_vocab, tgt_vocab)\n",
    "    print(f'English: {sample_sentence}')\n",
    "    print(f'Translation: {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
